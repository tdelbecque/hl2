\documentclass[a4paper,11pt]{report}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{geometry}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=blue,  %choose some color if you want links to stand out
}

\geometry{margin=2cm}

\lstset {
  basicstyle=\scriptsize
}
\newcommand{\cs}[1]{{\bfseries \ttfamily #1}}
\newcommand{\datadir}[1]{\cs{\$DATADIR/#1}}
\newcommand{\arrow}[2]{#1 $\rightarrow$ #2}
\newcommand{\fsig}[3]{\textcolor{blue}{\cs{#1::\arrow{#2}{#3}}}}
\newcommand{\sigit}[3]{\item[\fsig{#1}{#2}{#3}]}
\newcommand{\TODO}[1]{\begin{center}\bfseries \colorbox{yellow}{\parbox{0.9\textwidth}{TODO: #1}}\end{center}}
\newcommand{\WARNING}[1]{\begin{center}\bfseries \colorbox{red}{\parbox{0.9\textwidth}{WARNING: #1}}\end{center}}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\title{Technical Documentation \& HOW-TO}

\begin{document}

\maketitle
\tableofcontents
\chapter{Utilities}
\section{External resources}
\begin{itemize}
\item TurboParser
\item tree-tagger
  \begin{itemize}
  \item english-abbreviations
  \item english-utf8.par
  \item english-chunker-utf8.par
  \item filter-chunker-output.perl
  \end{itemize}
\item PERL
\item ES6/NodeJS : xpath, xmldom, http, https
\end{itemize}
\section{Data files organisation}
all data files go somewhere under the data directory.
\begin{description}
\item[data] root directory for the data files;
\item[data/journals] contains informations on journals (titles, category) and an sql script to get this information into a DB.
\item[data/FP] FPE output; it may be moved to data/in/FP and renamed to fingerprint to mirror the programs directories namings;
\item[data/omniscience] Omniscience vocabulary; it may be moved to data/in/omniscience, and renamed to fingerprint to mirror the program directory namings;
\item[data/in/HL] where all the extracted HL files go;
\item[data/out] today temporary files just go there, though it would be cleaner to use a data/tmp subdirectory;
\item[data/out/export] where HL generated for Rolf go;
\item[data/out/parser] outputs of TurboParser;
\item[data/out/predicates] this is missing today, but we should puts predicates here: now we just output predicates to the server, which is not quite enough;
  \item[data/out/www-resources] xml fragments for the server;
\end{description}

\section{Program files organisation}
The programs are organised in these directories:
\begin{description}
\item[scripts] mainly PERL programs that handle the NLP pipeline: tokenization, tagging, etc. This goes from the point where the HL are downloaded to the point where the SVO triples are extracted. There are also some programs used merely for analysis.
\item[scripts/network] NodeJS programs to extract HL from SD site, and to create a WWW proxy server to SD, serving the triples.
\item[scripts/fingerprint] NodeJS programs that leverage FPE outputs. In particular, tokenization taking into account FPE.
\item[scripts/ontology] NodeJS programs to manage OmniScience vocabulary: loading and browsing utilities.
\item[scripts/utils] NodeJS generaly helpful functions.
\item[TurboParser/HL] contains the programs to produce files to EPFL (see the make.sh file there), but it should be deprecated in the future.
\item[sql]
\item[nlm]
\end{description}
\subsection{scripts}
\subsubsection{make.sh}
\TODO{put the process in crontab. This means thas we must compute the diff between what is newly available and was has already been computed} 
this implements the NLP pipeline, going from an HL file to a prepared HTML-tagged fragments for the www server. Invocation is simple, for example:
%
\begin{lstlisting}[language=bash]
./make.sh HL.1489640401
\end{lstlisting}
%
The argument is the file to process, and it must lie in the data/in/HL directory. Several artifacts and outputs are produced. The main outputs are
%
\begin{itemize}
\item a TurboParser pred file in data/out/parser;
\item an xml file in data/out/www-resources;
\end{itemize}
%
As this is an integrated procedure, we may link it to the crontabed HL fetching process.
%
\subsubsection{extractNewHL.hs}
\cs{extractNewHL hldir paserdir}

this extracts the HL from the files in \cs{hldir} for which no parsing is found in \cs{parserdir}
%
\subsubsection{tokenize.pl}
\WARNING{This should be deprecated in favor of the FP aware tokenizer}
Output of tokenization will look like the following:
\begin{lstlisting}[language=XML]
  <PAPER PII="S0001457517301008">
  <HL>
  Rainfall
  increases
  the
  likelihood
  of
  single-vehicle
  (
  SV
  )
  crashes
  on
  mountainous
  highways
  .
  </HL>
\end{lstlisting}
Inside each \cs{<PAPER>} element there will be as many \cs{<HL>} elements as there are HL for the paper.
\subsubsection{tagchunks.pl}
TreeTagger is used also to perform a shallow analysis (chunk analysis). This process tags chunks to extract special informations such as the mode of the verbs and so on.
\subsubsection{strip-heading.pl}
This removes mundane headings.
\subsubsection{strip-terminators.pl}
This removes mundane tails.
\subsubsection{addTokno.pl}
\subsubsection{format4Turbo.pl}
\subsubsection{build-predicates.pl}
\subsubsection{addPredToTreeTagger.pl}
\subsubsection{tags2Hungs.hs}
This process takes a data in stdin and outputs on stdout. The data in is a file is a file as the one created by \cs{addPredToTreeTagger}. The output is suitable for indexing in Lucene as required by Hung.
\subsubsection{prepareHL4Server.pl}
This process uses stdin and stdout. 
It creates the HTML elements that can go in place of the HL in the presentation page. Input file should be the one produced from addPredToTreeTagger.pl. Following is a example of output from this script:
\begin{lstlisting}[language=HTML]
  <div class="abstract svAbstract abstractHighlights" data-etype="ab">
  <h4 id="absSec_1">Highlights</h4><p>
  <dl class="listitem">
  <dt class="label">&bull;</dt>
  <dd><p> <span class="sub">Two polysaccharide fractions</span>
  <span class="pred">were purified</span>
  <span class="obj">from roots of Gentiana crassicaulis .</span></p>
  </dd>
  <dt class="label">&bull;</dt>
  <dd><p> <span class="sub">These two fractions</span>
  <span class="pred">were</span>
  <span class="obj">pectic polysaccharides .</span></p>
  </dd>
  <dt class="label">&bull;</dt>
  <dd><p> <span class="sub">These two polysaccharides</span>
  <span class="pred">could be used</span>
  <span class="obj">as a potential natural immunomodulator .</span></p>
  </dd>
  </dl>
  </p></div>
\end{lstlisting}
\subsubsection{countActiveVerbs.pl}
\subsubsection{count-vc.pl}
\subsubsection{extract-verbs.pl}
\subsubsection{weshowthat.pl}
%
% scripts/network
%
\subsection{scripts/network}
\subsubsection{do.sh}
This script is regularly called by crontab. It extract fresh HL's from SD site, as is examplified in \ref{fetchinginet}.


\subsubsection{runGetHLPar.sh}
\subsubsection{functional.js}
\subsubsection{getUnknownPiis.js}
\subsubsection{getKnownPiis.js}
\subsubsection{getpii.js}
\subsubsection{listPIIS.js}
\subsubsection{runGetPiis.js}
\subsubsection{gethl.js}
\subsubsection{getpage.js}
\subsubsection{loadHL.js}
\subsubsection{runGetHL.js}
\subsubsection{runGetHLPar.js}
\subsubsection{server.js}
\subsubsection{extractPaperFeatures.js}
extract informations from a file in pages-xml.
\TODO{FINISH IT}
\subsubsection{extractPaperFeatures.hs}
extract informations from a file in pages-xml. (Haskell version)
\TODO{FINISH IT}
\subsection{scripts/fingerprint}
%
\subsubsection{json-utils.js}
This will be deprecated, use utils/utils.js instead.
%
\subsubsection{digest.js}
\WARNING{This module digests FPE in xml format, one file per pii. The pii is encoded in the name of the file}
\WARNING{This may be deprecated, as it seems that FPE output are to come in delimited form. See \cs{digest-delimited} instead}
The digest module loads all FPE outputs found in a given directory and subdirectory.
The default directory is \datadir{FP}. The main methods are
%
\begin{description}
\sigit{load}{path\_to\_directory}{Promise} loads the FPE results found in a directory, recursively, and populates the internal \cs{dict} object. The rule is that the names of the files must contain a PII (re = S(X|$\backslash$d)\{16\}) and have .xml as an extension.
\sigit{output}{()}{()} output the loaded FPE to stdout in a tabulated format. The file is tab delimited with a header, and the column names are:
  \begin{enumerate}
  \item pii
  \item ConceptID
  \item TermID
  \item TermType
  \item Text
  \item TextEnd
  \item TextOffset
  \item Thesaurus
  \end{enumerate}
\end{description}
%
The \cs{dict} attribute is an object where keys are the PII's, and values are arrays of FPE terms. For example:
\begin{lstlisting}[language=java]
  const digest = require ('./digest')
  const f = new digest ()
  f.load ().then (sayOk, croak)
\end{lstlisting}
when the promise is fulfilled, \cs{f.dict} values will be arrays of objects of the following format:
\begin{lstlisting}[language=java]
   { ConceptID: '212071512',
    TermID: 'A231885',
    TermType: 'NonPrefLabel',
    Text: 'beneficial',
    TextEnd: '361',
    TextOffset: '351',
    Thesaurus: 'OmniscienceAgriBio' }
\end{lstlisting}
\subsubsection{digest-delimited.js}
\subsubsection{tokenize.js}
This is the next tokenizer that should replace script/tokenize.pl. It leverages the output of FPE as well as the vocabulary in the following way:
\begin{itemize}
\item keep the FP terms as a single token;
\item use the concept id of the term as the lemma;
\item The Pos tag for FPE tags is either NN or NNS, according to weither or not the term is plural.
\end{itemize}
The synopsis for using the tokenizer is:
\begin{lstlisting}[numbers=left, language=java]
  // node must be run with a big amount of memory,
  // for example
  // node --max-old-space-size=16384
  const m = require ('./tokenize')
  const f = new m ()
  f.load ().then (\_\ =>\ f.tokenize ()) 
\end{lstlisting}
The main methods of \cs{tokenize} are:
%
\begin{description}
  \item[\cs{constructor (path\_to\_FP\_directory)}]
  \sigit{load}{path\_to\_FP\_file}{Promise} loads FPE files (currently from a default directory), and an HL data file. When the promses are fulfilled the \cs{JOIN} internal map is populated as shown below.
  \sigit{verify}{()}{()} output on stdout terms of FPE tagging face to face with the text fragment found in the HL. This fragment is computed from the \cs{TextOffset} and \cs{TextEnd} attributes of the FPE data.
  \sigit{tokenize}{()}{()} tokenize the HL data and update the \cs{JOIN} internal map accordingly.
\end{description}
%
The \cs{JOIN} internal map has PII's as keys. Values arrays made of :
\begin{enumerate}
\item the HL data
\item the FPE data
\item the tokenizer data
\end{enumerate}
%
\subsection{scripts/ontology}
\subsubsection{loadOmni.js}
This load a vocabulary in a simple array.
\begin{description}
\item[data]
\item[fields]
\end{description}
\begin{description}
\item[load]
\item[test]
\item[lookFor]
\item[lookForRelationType]
\item[prefLabel]
\item[isPreflabelFor]
\item[moreGeneral]
\item[moreSpecific]
\item[dumpLabels]
\end{description}
\TODO{make the source file not by default}
\subsubsection{loadOmniMaps.js}
\cs{loadOmniMap} is a wrapper around \cs{loadOmni}. \cs{load} methods returns a \cs{Promise}. When loading is over, the following \cs{Map}'s are populated:
%
\begin{description}
  %
\sigit{prefLabelForMap}{UID}{\{UID, label::String\}}. For each ConceptUID (the key) retrieves the \cs{(ConceptUID, label)} of the preflabel node
  %
\sigit{isPrefLabelForMap}{label::String}{\{UID, for::Set[UID]\}}. For a label given as a string, retrieves the corresponding prefLabel node, and the set of concept nodes this label is a prelLabel for.
\sigit{generalForMap}{UID}{Set[UID]}, climbs one step up the hierarchy
\sigit{specificForMap}{UID}{Set[UID]}, climbs one step down the hierarchy.
\end{description}
%
The wrapped omni vocabulary can be reached with the \cs{omni} field.
\subsubsection{compareFPWithOmniscience.js}
Compare concepts found in an FPE result with the concepts of an Omniscience vocabulary.
\subsection{scripts/utils}
\subsubsection{utils.js}
\begin{itemize}
\item\cs{say = x =>}
\item\cs{croak = x =>}
\item\cs{sayOk = () =>}
\item\cs{croakFail = () =>}
\item\cs{definedOr = (x, y) =>}
\item\cs{tabulate = (xs, field) =>}
\item\cs{fswitch = alternatives =>}
\item\cs{isNothing = x =>}
\item\cs{isSomething = x =>}
\item\cs{identity = x => x}
\item\cs{isWeaklyTrue = x =>}
\item\cs{firstF = (fs, x, test=isSomething) =>}
\item\cs{firstA = (xs, f, test=isSomething) =>}
\item\cs{loadDictionary = (file, handler) =>}
\item\cs{flatten = xs =>}
\item\cs{stringify = data =>}
\item\cs{assert = (condition, exception) =>}
\item\cs{assertNot = (condition, exception) =>}
\end{itemize}
\subsubsection{pg.js}
Class with the following methods:
\begin{itemize}
\item\cs{constructor (connectionString)}
\item\cs{getClient ()}
\item\cs{async connect ()}
\item\cs{async query (q)}
\item\cs{end ()}
\end{itemize}
\subsubsection{async.js}
\begin{itemize}
\item \cs{async function lstat (fn)}
\item \cs{async function lsdir (dn)} given the path to a directory returns a list of files in this directory.
\item \cs{async function readFile (file, options={encoding:"utf8"})}
\item \cs{async function assertDirectory (path)}
\end{itemize}
\subsubsection{fs.js}
\begin{itemize}
  \item \cs{async function readLines (file, handler)} reads a file line by line; handler must have a addLinesAsync method that will be fed with list of new lines. Return from this method determines whether reading the file should continue or not : true means ending. 
\end{itemize}
\subsection{script/nlm}
Here should come the programs that leverage NLM resources. For now, there is only somethng to load the english index in memory.
\begin{description}
\item[loadDctionnaryDelimited] note: this is a good example showing how to use \cs{loadDictionary} function of the \cs{util} package.
\end{description}
\section{fingerprint}
\subsection{digest}
This exports an object with two methods: \emph{load} and \emph{output}. \emph{output} just output on stdin the content of the retrieved FingerPrints in a tabulated format.
\subsubsection{load}
$o.load\ ::path\Rightarrow\ Promise$

The path is to any directory; the method browses recursively this directory to find files that contain a PII in their name, and parse these files for FP content. Whenthe promise is fulfilled, the \emph{dict} member of $o$ contains the retrieved FPs:
$dict::Map\ PII\ [FP]$
where FP structure contains the following fields:
\begin{itemize}
\item ConceptID
\item TermID
\item TermType
\item Text
\item TextEnd
\item TextOffset
\item Thesaurus
\end{itemize}


\section{scripts/network}
\subsection{loadHL}
\subsubsection{loadMap}
$
loadMap\ ::\ path\rightarrow Promise 
$

When fulfilled, the Promise delivers a dictionnary $PII\Rightarrow HLset$, where $HLset$ is indeed a string of HL separatd with ``' • '''. Note that ``' • ''' is also present at the very beginning of the string, and that the value associated with a PII without HL would just be ``' • '''.

An example of usage can be found in \emph{server.js}.

\section{Fetching data on inet}\label{fetchinginet}
The code related to this section is in directory \emph{scripts/network}.

Fetching data is a two steps process:
\begin{enumerate}
\item visiting ScienceDirect with its API to get a lst of recently published papers;
\item for each new paper, vsiting SD to extract HLs, if there is some HL in the paper.
\end{enumerate}


This whole process is implemented in the \emph{do.sh} little script, also displayed in the following listing:
\lstinputlisting[numbers=left,language=bash,basicstyle=\scriptsize]{../scripts/network/do.sh}

\begin{description}
\item[line 13] is where SD is visited to get the fresh PII's. In this calling mode (that is, without any argument), the API is used to retrieve PII's of papers published yesterday; otherwise it is possible to call \emph{runGetPiis} with date range as parameters;
\item[line 14] is where we check for the PII's that are unknown, in the sense that no HL retrieval try has ben run yet for them (otherwise the PII would appear somewhere either in the HL directory, or in the \emph{pbaccess} file computed line 12);
\item[line 15] is where we try to extract HL for each unknown PII. Note that even thos PII's for which no HL where found are recorded in the HL directory, with an empty HL list.
\end{description}

\section{Web Server}
It is implemented in \emph{server.js} file. 

\TODO{all xml files should be used: browse a directory.}

\chapter{Postprocessing}
\section{POS tagging postprocessing}
It aims at fixing some errors made by TreeTagger. We assume that TreeTagger is likely to have failed when no predicate was found, meaning that TurboParser was not able to find ROOT, SUB and OBJ nodes in the dependency tree.

The principle is to use all HL that could be translated into predicates (so where TreeTagger was successful) as a reference set from which some statistics will be extracted. These statstics will be used to pre-set some tags in the HL from the failed set.

Often the predicate could not be extracted because the verbs were not correctly tagged. So this algorithm focuses on verbs and tries to fix potentiel tag errors there.
\subsection*{Scoring formula}
Error correction is based on the prediction of the tag given the token and the context. The context is made of the POS just before and just after the element under inspection. The new tag $tag\prime$ is defined as:
$$
tag\prime = argmax_{tag} P(tag|context,token)
$$
rewriting the probabilities:
$$
P(tag|context,token)=\frac{P(context,token|tag)P(tag)}{P(context,token)}
$$
We do not want to use the estimate of $P(context,token|tag)$ from the dataset as this is where the tagger failed, and doing so would just reinject the error we want to fix in the procedure. So we replace it with $P(context|tag)P(token|tag)$: we make the assumption that the errors made by the tagger is more diluted if we use these elementary probabilities. So we now have:
$$
tag\prime = argmax_{tag}\left(\frac{P(context|tag)P(token|tag)P(tag)}{P(context,token)}\right)
$$
By pluging in the counts:
$$
tag\prime = argmax_{tag}\left(\frac{\#(context,tag)}{\#tag}\frac{\#(token,tag)}{\#tag}\frac{\#tag}{N}\frac{N}{\#(context,token)}\right)
$$
so:
$$
tag\prime = argmax_{tag}\left(\frac{\#(context,tag) \#(token,tag)}{\#tag \#(context,token)}\right)
$$
The following tables implements the distinct parts of this formula:
\begin{description}
\item[F1] $\equiv \#(context,tag)$
\item[F2] $\equiv \#(token,tag)$
\item[F3] $\equiv \#tag$
\item[F4] $\equiv \#(context,token)$
\item[F5] $\equiv \frac{F1\times F2}{F3\times F4}$
\end{description}
\subsection*{Process}
We are in \cs{script/analysis}.
\begin{enumerate}
\item format the predicate files:
  \begin{lstlisting}[language=Bash]
    perl export-tokens.pl <
      ~/HL/data/out/predicates/PENDING.1495553761.tagged >
      tokens.tsv
  \end{lstlisting}
\item import into PG:
  \begin{lstlisting}[language=SQL]
    drop table if exists tokens;
    create table tokens (
       pii   char(17),
       hlno  integer,
       gtokno bigint,
       token text,
       pos   text,
       lemma text,
       tokno  integer,
       klass char(4));

    \copy tokens from 'tokens.tsv' with csv delimiter E'\t' quote E'\b';
    alter table tokens add primary key (gtokno);
  \end{lstlisting}
\item compute the context table
  \begin{lstlisting}[language=SQL]
    drop table if exists token_ctx_left;
    create table token_ctx_left as (
      select A.gtokno, A.pii, A.hlno, A.tokno, A.klass,
        B.pos posbef, A.pos, B.token tokbef , A.token
        from tokens A
        left outer join tokens B on A.gtokno = B.gtokno + 1);
    alter table token_ctx_left add primary key (gtokno);

    drop table if exists tokens_ctx1;
    create table tokens_ctx1 as (
      select A.*,  B.pos posaft, B.token tokaft
      from token_ctx_left A
      left outer join tokens B on B.gtokno = A.gtokno + 1);
      
    alter table tokens_ctx1 add primary key (gtokno);
    create index tokens_ctx1_tok_idx on tokens_ctx1(token);
    create index tokens_ctx1_pos_idx on tokens_ctx1(pos);
    create index tokens_ctx1_posbef_idx on tokens_ctx1(posbef);
    create index tokens_ctx1_posaft_idx on tokens_ctx1(posaft);

    update tokens_ctx1 set posbef='NIL', tokbef='' where tokbef is null;
    update tokens_ctx1 set posaft='NIL', tokaft='' where tokaft is null;
  \end{lstlisting}
\item Create the context table restricted to successful tagging
  \begin{lstlisting}[language=SQL]
    drop table if exists tokens_ctx1_in;
    create table tokens_ctx1_in as (
      select
        *,
        case when  posbef='IN' then 'IN_' || tokbef else posbef end statbef,
        case when posaft = 'IN' then 'IN_' || tokaft else posaft end stataft
      from tokens_ctx1
      where klass != 'unk');
  \end{lstlisting}
\item create the list of verbs: as we want to fix tagging problems that occured on verbs, we begin by extracting a list of candicate verbs:
  \begin{lstlisting}[language=SQL]
    \copy (
      select count(*) n, token
      from tokens
      where klass != 'unk' and
             pos in ('VBP', 'VBD', 'VBZ') and
             lemma not in ('be', 'do', 'can', 'have')
      group by token
      order by n desc)
    to verbs.tsv with csv delimiter E'\t';
  \end{lstlisting}
  \begin{lstlisting}[language=bash]
    perl -ne 'print "$1\n" if /^\d+\t([a-zA-Z][a-z]+)$/' < verbs.tsv > verbs
  \end{lstlisting}
  \begin{lstlisting}[language=SQL]
    drop table if exists verbs_curated;
    create table verbs_curated (token text);
    \copy verbs_curated from 'verbs';
  \end{lstlisting}
\item compute the scoring functions, F1 to F4 (\cs{create\_fun.sql}):
  \begin{lstlisting}[language=SQL]
    drop table if exists F3 cascade;
    create table F3 as (
      select count(*) n, pos
      from tokens_ctx1_in
      where pos in ('VBP', 'VBZ', 'VBD', 'NN', 'NNS', 'NP', 'NPS', 'JJ', 'RB')
      group by pos);

    drop table if exists F1 cascade;
    create table F1 as (
      select count(*) n, statbef, pos, stataft
      from tokens_ctx1_in A
      join verbs_curated B on A.token = B.token
      where pos in (select pos from F3)
      group by pos, statbef, stataft);
    alter table F1 add primary key (statbef, pos, stataft);
    create index F1_statbef_idx on F1(statbef);
    create index F1_pos_idx on F1(pos);
    create index F1_stataft_idx on F1(stataft);

    drop table if exists F2 cascade;
    create table F2 as (
      select count(*) n, A.token, pos
      from tokens_ctx1_in A
      join verbs_curated B on B.token = A.token
      where pos in (select pos from F3)
      group by A.token, pos);
    alter table F2 add primary key (token, pos);
    create index F2_token_idx on F2(token);
    create index F2_pos_idx on F2(pos);

    drop table if exists F4 cascade;
    create table F4 as (
      select count(*) n, statbef, A.token, stataft
      from tokens_ctx1_in A
      join verbs_curated B on B.token = A.token
      group by statbef, A.token, stataft);
    alter table F4 add primary key (statbef, token, stataft);
    create index F4_statbef_idx on F4(statbef);
    create index F4_token_idx on F4(token);
    create index F4_stataft_idx on F4(stataft);

    drop table if exists F5 cascade;
    create table F5 as (
      select
        (F1.n::real * F2.n)/(F3.n * F4.n) score,
        F1.statbef,
        F2.token,
        F3.pos,
        F1.stataft
      from F3
      join F1 on F3.pos = F1.pos
      join F2 on F2.pos = F3.pos
      join F4 on F1.statbef = F4.statbef and F1.stataft = F4.stataft and F4.token = F2.token);
  \end{lstlisting}
\item create a table with statistics that will allow us to take appart 'bad' hl according to our criteria:
  \begin{lstlisting}[language=SQL]
    -- this table is used to take apart analysis according to whether we believe
    --   they are successful or not.
    -- A successful analysis led to a 'predicate', in which case U <> size.
    drop table if exists success;
    create table success as (
      select pii, hlno, count(*) size,
             sum (case when klass='unk' then 1 else 0 end) U,
             sum (case when pos in ('VBP', 'VBZ', 'VBD') then 1 else 0 end) V
      from tokens group by pii, hlno);
  \end{lstlisting}
\item exports failed HL's:
  \begin{lstlisting}[language=SQL]
    drop table if exists fail_verbs cascade;
    create table fail_verbs as (
      select distinct pii, hlno
      from tokens A
      join verbs_curated B on A.token = B.token
      where klass='unk' and pos not like 'V%');

    drop table if exists bar cascade;
    create table bar as
      select distinct A.pii, A.hlno
      from success A
      join fail_verbs B on A.pii = B.pii and A.hlno = B.hlno where v = 0 and size=u;

    \copy (select X.pii, X.hlno, A.tokno,
                  case when B.posbef = 'IN' then 'IN_' || B.tokbef else posbef end statbef,
                  case when B.posaft = 'IN' then 'IN_' || B.tokaft else B.posaft end stataft,
                  A.pos, A.token, A.lemma
           from bar X
           join  tokens A on A.pii = X.pii and A.hlno = X.hlno
           join  tokens_ctx1 B on B.pii = X.pii and B.hlno = X.hlno and A.tokno = B.tokno
           order by X.pii, X.hlno, A.tokno)
        to 'failure.tsv' with csv delimiter E'\t';
  \end{lstlisting}
\item try to fix the tagging errors
  \begin{lstlisting}[language=bash]
    scala -J-Xmx24g  -cp "/home/thierry/lib/*:./fix-errors" Main < analysis/failures.tsv > bar
  \end{lstlisting}
\end{enumerate}

  
\chapter{HOWTOS}
\section{check list from epfl and update accordingly}
\begin{enumerate}
\item check piis that are not covered:
  \begin{lstlisting}[language=SQL]
    \copy (select distinct pii
    from (select A.pii , B.pii x from hughes A left outer
      join hung_predicates B on A.pii = B.pii) X where x is null) to 'UNKNOWNS';
  \end{lstlisting}
\item get the data:
  \begin{lstlisting}
    node runGetHLPar.js UNKNOWNS > ~/HL/data/in/HL/HL.patch-hughes-2
  \end{lstlisting}
\item compute new candidates HL and analyse
  \begin{lstlisting}
    ./extractNewHL ~/HL/data/in/HL/ ~/HL/data/out/parser/ | sort -k 1,1 -u > ~/HL/data/out/PENDING.hughes-2
    ./make.sh ~/HL/data/out/PENDING.hughes-2 2>network/LOGS/make.hughes-2
  \end{lstlisting}  
\end{enumerate}

\section{Create samples for experiment 1}
\subsection{Generated files}
In the first step:
\begin{itemize}
\item sample-meta-proposal.tsv
\item sample-meta-we-proposal.tsv
\item sample-meta-study-proposal.tsv
\item sample-factual-proposal.tsv
\item sample-simple-proposal.tsv
\item sample-multi-and-proposal.tsv
\item sample-multi-notand-proposal.tsv
\item sample-nontechnical-proposal.tsv
\item sample-midlytechnical-proposal.tsv
\item sample-highlytechnical-proposal.tsv
\end{itemize}

These files where send in a first shot in the form of excel files. A second shot contained the full hl set. This second shot is stored in the \cs{with-hl.d} directory.

\subsection*{Compute statistical features of HLs}
Use the program updatedb-stats.js to extract the following data:
\begin{itemize}
\item pii
\item hlno
\item ntokens
\item nsub
\item npred
\item nobj
\item nunk
\item nnouns
\item nconcepts
\item ndistconc
\item nverbs
\item nverbsstrong
\item nverbsunknown
\item hl
\end{itemize}
This program browses the the predicate directory for the tagged files, and outputs a tab delimited file.

This file is pg-imported in the \cs{hlstatistics}. Extrainfo are added in \cs{hlstatisticsfull}:
\begin{lstlisting}[language=bash]
  node --max-old-space-size=24596 updatedb-stats.js > /home/thierry/HL/data/out/hlstats.2
\end{lstlisting}
then
\begin{lstlisting}[language=SQL]
  drop table if exists hlstatistics;
  create table hlstatistics (
       PII char(17),
       hlno int,
       ntokens int,
       nsub int,
       npred int,
       nobj int,
       nunk int,
       nnouns int,
       nconcepts int,
       ndistconc int,
       nverbs int,
       nverbsstrong int,
       nverbsunknown int,
       hl text);

    \copy hlstatistics from '/home/thierry/HL/data/hlstats.2' with csv delimiter E'\t' quote E'\b' header;
   alter table hlstatistics add primary key(pii, hlno);

   drop table if exists hlstatisticsfull;
   create table hlstatisticsfull as (
     select A.*, B.issn, C.journal_title, D.category
     from hlstatistics A
     join articles B on A.pii = B.pii
     join journals_title C on B.issn = c.issn
     join journals_categories D on D.issn = B.issn);
\end{lstlisting}
We add some statistics about common words. For this we check the presence of the words in an american common dictionary
\begin{lstlisting}[language=bash]
  perl ../check-dictionary.pl < ~/HL/data/hlstats.2 > ~/HL/data/hlstats.dicverified
\end{lstlisting}
And we pg-import this:
\begin{lstlisting}[language=SQL]
  drop table if exists hlstatistics_dic;
  create table hlstatistics_dic (
       PII char(17),
       hlno int,
       ntokens int,
       nsub int,
       npred int,
       nobj int,
       nunk int,
       nnouns int,
       nconcepts int,
       ndistconc int,
       nverbs int,
       nverbsstrong int,
       nverbsunknown int,
       hl text,
       dicknown int,
       dicunknown int);
   \copy hlstatistics_dic from '/home/thierry/HL/data/hlstats.dicverified' with csv delimiter E'\t' quote E'\b' header;
\end{lstlisting}
Journals from Cell are set in a table:
\begin{lstlisting}[language=SQL]
  drop table if exists celljournals;
  create table celljournals (
    issn char(8),
    journal_title text);
  \copy celljournals from '/home/thierry/HL/data/cellrolf.txt' with csv delimiter E'\t' quote E'\b';
\end{lstlisting}
We create tables specific to Cell:
\begin{lstlisting}[language=SQL]
  drop table if exists hlstatisticscell;
  create table hlstatisticscell as (
    select A.*
    from hlstatistics A
    join articles C on C.pii = A.pii join
    celljournals B on C.issn = B.issn);
    
  alter table hlstatisticscell add column rnd float;
  update hlstatisticscell set rnd = random ();

  drop table if exists hlstatisticscell_dic;
  create table hlstatisticscell_dic as (
    select A.*, dicknown, dicunknown
    from hlstatisticscell A
    join hlstatistics_dic B on A.pii = B.pii and A.hlno = B.hlno);
\end{lstlisting}
\subsection*{Extractions}
\subsubsection*{Meta}
\begin{lstlisting}[language=SQL]
  \copy (select pii, hlno, hl
    from hlstatisticscell where nverbsstrong = 1  and nunk > 0 order by rnd)
  to 'meta.tsv' with csv delimiter E'\t';
\end{lstlisting}
We somehow extract terminations in 'ed' that we put in \cs{meta-ends}. Then
\begin{lstlisting}{language=bash}
  # WTF ?
  grep " we " meta.tsv > sample-meta.tsv
  grep " study " meta.tsv >> sample-meta.tsv
  perl extract-meta.pl < meta.tsv >> sample-meta.tsv
\end{lstlisting}

\begin{lstlisting}[language=SQL]
  \copy (select pii, hlno, hl
    from hlstatisticscell
    where hl like 'We %' and nverbsstrong = 1 order by rnd)
  to 'meta-we.tsv' with csv delimiter E'\t';
  \copy (select pii, hlno, hl
    from hlstatisticscell
    where hl like '% study %' and nverbsstrong < 2 order by rnd)
  to 'meta-study.tsv' with csv delimiter E'\t';

  drop table if exists sample_meta;
  create table sample_meta (
       pii char(17),
       hlno int,
       hl text);
  \copy sample_meta from 'sample-meta.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
    from sample_meta A
    join articles B on A.pii = B.pii
    join celljournals C on B.issn = C.issn
    join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-meta-proposal.tsv' with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
  \copy sample from 'meta-we.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
    from sample A
    join articles B on A.pii = B.pii
    join celljournals C on B.issn = C.issn
    join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-meta-we-proposal.tsv' with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
  \copy sample from 'meta-study.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
    from sample A
    join articles B on A.pii = B.pii
    join celljournals C on B.issn = C.issn
    join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-meta-study-proposal.tsv' with csv delimiter E'\t';
\end{lstlisting}

\subsubsection*{Factuals}
\begin{lstlisting}[language=SQL]
  \copy (select pii, hlno, hl
     from hlstatisticscell
     where nunk = 0 and
     hl not like 'We %' and hl not like '%ed .%' and
     rnd < 0.01 order by rnd)
   to 'factual.tsv' with csv delimiter E'\t';
   -- why exporting and importing ? wtf ?
   drop table if exists sample;
   create table sample (
       pii char(17),
       hlno int,
       hl text);
   \copy sample from 'factual.tsv' with csv delimiter E'\t';
   \copy (select A.*, title, C.journal_title
      from sample A
      join articles B on A.pii = B.pii
      join celljournals C on B.issn = C.issn
      join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
    to 'sample-factual-proposal.tsv' with csv delimiter E'\t';
\end{lstlisting}
\subsubsection*{Simples}
\begin{lstlisting}[language=SQL]
  \copy (select pii, hlno, hl
     from hlstatisticscell
     where nunk = 0 and hl not like 'We %' and nverbsstrong = 1 and rnd> 0.98 order by rnd)
   to 'simple.tsv' with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
  \copy sample from 'simple.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
    from sample A join articles B on A.pii = B.pii
    join celljournals C on B.issn = C.issn
    join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-simple-proposal.tsv' with csv delimiter E'\t';   
\end{lstlisting}

\subsubsection{Multiples}
\begin{lstlisting}[language=SQL]
  \copy (select pii, hlno, hl
    from hlstatisticscell
    where nunk = 0 and hl not like 'We %' and hl like '% and %' and nverbsstrong > 2
    order by ntokens desc, rnd)
  to multi-and.tsv with csv delimiter E'\t';

  \copy (select pii, hlno, hl
    from hlstatisticscell
    where nunk = 0 and hl not like 'We %' and hl not like '% and %' and nverbsstrong > 2
    order by ntokens desc, rnd)
  to multi-notand.tsv with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
   \copy sample from 'multi-and.tsv' with csv delimiter E'\t';
   \copy (select A.*, title, C.journal_title
         from sample A join articles B on A.pii = B.pii
         join celljournals C on B.issn = C.issn
         join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
     to 'sample-multi-and-proposal.tsv' with csv delimiter E'\t';

   drop table if exists sample;
   create table sample (
       pii char(17),
       hlno int,
       hl text);
   \copy sample from 'multi-notand.tsv' with csv delimiter E'\t';
   \copy (select A.*, title, C.journal_title
     from sample A
     join articles B on A.pii = B.pii
     join celljournals C on B.issn = C.issn
     join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
   to 'sample-multi-notand-proposal.tsv' with csv delimiter E'\t';
\end{lstlisting}

\subsubsection*{Specialized}
\begin{lstlisting}[language=SQL]
  \copy (select pii, hlno, hl
    from hlstatisticscell_dic
    where dicunknown::float = 0  and nverbsstrong > 1 and rnd < 0.6 order by rnd)
  to 'nontechnical.tsv' with csv delimiter E'\t';

  \copy (select pii, hlno, hl
    from hlstatisticscell_dic
    where dicunknown::float / dicknown > 1 and ndistconc > 0 and nverbsstrong > 1 order by rnd)
  to 'highlytechnical.tsv' with csv delimiter E'\t';

  \copy (select pii, hlno, hl
    from hlstatisticscell_dic
    where dicunknown::float / dicknown < 0.3 and dicunknown::float / dicknown > 0.2
      and ndistconc > 0 and nverbsstrong > 1 and rnd < 0.4 order by rnd)
  to 'midlytechnical.tsv' with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
  \copy sample from 'nontechnical.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
         from sample A
         join articles B on A.pii = B.pii
         join celljournals C on B.issn = C.issn
         join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-nontechnical-proposal.tsv' with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
  \copy sample from 'midlytechnical.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
    from sample A
    join articles B on A.pii = B.pii
    join celljournals C on B.issn = C.issn
    join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-midlytechnical-proposal.tsv' with csv delimiter E'\t';

  drop table if exists sample;
  create table sample (
       pii char(17),
       hlno int,
       hl text);
  \copy sample from 'highlytechnical.tsv' with csv delimiter E'\t';
  \copy (select A.*, title, C.journal_title
    from sample A
    join articles B on A.pii = B.pii
    join celljournals C on B.issn = C.issn
    join hlstatisticscell D on D.pii = A.pii and D.hlno = A.hlno order by D.rnd)
  to 'sample-highlytechnical-proposal.tsv' with csv delimiter E'\t';
\end{lstlisting}

\chapter{Tools from the NLM}
\section{Resources}
They are organized in the \$UMLSPATH directory:
\begin{description}
\item[\$UMLSPATH/2016AB] the MetaThesaurus;
\item[\$UMLSPATH/lvg2017] the SPECIALIST lexicon tools
\end{description}

\section{SPECIALIST lexicon tools}
Caveat: need to change \cs{gtar} to \cs{tar} in the installation script.

Calling \cs{norm} can be used to normalize terms from Omniscience vocabulary and retrieve them in the \cs{MRXNS\_ENG.RRF} index. we may use \cs{lvg} to generate variants for the prefferred labels.

It is possible to output all prefLabels of the vocabulary with the \cs{ontology/
outputPrefLabels.js} program:
\begin{lstlisting}[language=bash]
  node outputPrefLabels.js > prefLabels.txt
\end{lstlisting}
and then to normalize all the preflabels:
\begin{lstlisting}
  norm -i:prefLabels.txt -o:prefLabels\_norm.txt
\end{lstlisting}
\end{document}
